{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# All Imports\n",
    "\n",
    "import numpy as np\n",
    "#import astra\n",
    "import os\n",
    "import imageio\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "from numba import cuda\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import glob\n",
    "from skimage import filters\n",
    "from skimage.filters import unsharp_mask, threshold_local, threshold_minimum, threshold_otsu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import label\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "import pywt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "import pywt\n",
    "\n",
    "import scipy.io as sio\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "\n",
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import skimage.io as io\n",
    "import collections, numpy\n",
    "import warnings\n",
    "from scipy import ndimage, misc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import ipyvolume as ipv\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import numpy as np\n",
    "from ipdb import set_trace as bp\n",
    "import pandas as pd\n",
    "\n",
    "import numpy\n",
    "import warnings\n",
    "import functools\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0,
     13,
     163
    ]
   },
   "outputs": [],
   "source": [
    "# All CNN Models\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 32\n",
    "        filter2 = 64\n",
    "        filter3 = 128\n",
    "        filter4 = 256\n",
    "        filter5 = 512\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MyUnet_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        filter1 = 16\n",
    "        filter2 = 32\n",
    "        filter3 = 64\n",
    "        filter4 = 128\n",
    "        filter5 = 256\n",
    "    \n",
    "        self.dense_block = nn.Sequential(nn.Linear(1, 128),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(128, 32),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         nn.Linear(32, 1),\n",
    "                                         nn.LeakyReLU()\n",
    "                                        )\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter5, filter5, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.upsample1   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample3   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample4   = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        self.conv_block_merge1 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge2 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge3 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block_merge4 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(filter5, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter4, filter4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(filter4, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter3, filter3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(filter3, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter2, filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block9 = nn.Sequential(\n",
    "            nn.Conv2d(filter2, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(filter1, filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        self.conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(filter1, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        hash_val = self.dense_block(y)\n",
    "        hash_val = hash_val.view(-1, 1, 1, 1)\n",
    "        \n",
    "        x1 = self.conv_block1(x)\n",
    "        x1 = torch.mul(x1, hash_val)\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(p1)\n",
    "        x2 = torch.mul(x2, hash_val)\n",
    "        p2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(p2)\n",
    "        x3 = torch.mul(x3, hash_val)\n",
    "        p3 = self.pool3(x3)\n",
    "        #p3 = self.d1(p3)\n",
    "        \n",
    "        x4 = self.conv_block4(p3)\n",
    "        x4 = torch.mul(x4, hash_val)\n",
    "        p4 = self.pool4(x4)\n",
    "        #p4 = self.d2(p4)\n",
    "        \n",
    "        x5 = self.conv_block5(p4)\n",
    "        x5 = torch.mul(x5, hash_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        u1 = self.upsample1(x5)\n",
    "        m6 = self.conv_block_merge1(u1)\n",
    "        m6 = torch.cat((x4, m6), 1)\n",
    "        x6 = self.conv_block6(m6)\n",
    "        x6 = torch.mul(x6, hash_val)\n",
    "        \n",
    "        u2 = self.upsample2(x6)\n",
    "        m7 = self.conv_block_merge2(u2)\n",
    "        m7 = torch.cat((x3, m7), 1)\n",
    "        x7 = self.conv_block7(m7)\n",
    "        x7 = torch.mul(x7, hash_val)\n",
    "        \n",
    "        u3 = self.upsample3(x7)\n",
    "        m8 = self.conv_block_merge3(u3)\n",
    "        m8 = torch.cat((x2, m8), 1)\n",
    "        x8 = self.conv_block8(m8)\n",
    "        x8 = torch.mul(x8, hash_val)\n",
    "        \n",
    "        u4 = self.upsample4(x8)\n",
    "        m9 = self.conv_block_merge4(u4)\n",
    "        m9 = torch.cat((x1, m9), 1)\n",
    "        x9 = self.conv_block9(m9)\n",
    "        x9 = self.conv_block10(x9)\n",
    "        \n",
    "        out = torch.sub(x, x9)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# For reading the training data for WalNut Dataset\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "train_list = np.arange(1, 29)\n",
    "val_list   = np.arange(30, 34)\n",
    "test_list  = np.arange(35, 43)\n",
    "\n",
    "train_size = len(train_list)*200\n",
    "x_array = np.zeros([train_size, 1, 256, 256], dtype='float16')\n",
    "v_array = np.zeros([train_size, 1], dtype='float32')\n",
    "y_array = np.zeros([train_size, 1, 256, 256], dtype='float16')\n",
    "\n",
    "val_size = len(val_list)*200\n",
    "x_val_array = np.zeros([val_size, 1, 256, 256], dtype='float16')\n",
    "v_val_array = np.zeros([val_size, 1], dtype='float32')\n",
    "y_val_array = np.zeros([val_size, 1, 256, 256], dtype='float16')\n",
    "\n",
    "patches_path = '/media/data/WalnutDataRecon/'\n",
    "count        = 0\n",
    "algorithm    = 'huber'\n",
    "\n",
    "for t in train_list:\n",
    "    x = np.load(patches_path+'x-array-'+str(t)+'-'+algorithm+'.npy')\n",
    "    y = np.load(patches_path+'y-array-'+str(t)+'-'+algorithm+'.npy')\n",
    "    v = np.load(patches_path+'v-array-'+str(t)+'-'+algorithm+'.npy')\n",
    "    \n",
    "    y_array[count: count+x.shape[0], :, :, :] = y\n",
    "    x_array[count: count+x.shape[0], :, :, :] = x\n",
    "    v_array[count: count+x.shape[0], :]       = v\n",
    "    count = count+x.shape[0]\n",
    "    #print(t, v.shape)\n",
    "\n",
    "x_array = x_array[:count]\n",
    "y_array = y_array[:count]\n",
    "v_array = v_array[:count]\n",
    "print(x_array.shape, y_array.shape, v_array.shape, count)\n",
    "\n",
    "count = 0\n",
    "for t in val_list:\n",
    "    x = np.load(patches_path+'x-array-'+str(t)+'-'+algorithm+'.npy')\n",
    "    y = np.load(patches_path+'y-array-'+str(t)+'-'+algorithm+'.npy')\n",
    "    v = np.load(patches_path+'v-array-'+str(t)+'-'+algorithm+'.npy')\n",
    "    \n",
    "    y_val_array[count: count+x.shape[0], :, :, :] = y\n",
    "    x_val_array[count: count+x.shape[0], :, :, :] = x\n",
    "    v_val_array[count: count+x.shape[0], :]       = v\n",
    "    count = count+x.shape[0]\n",
    "\n",
    "x_val_array = x_val_array[:count]\n",
    "y_val_array = y_val_array[:count]\n",
    "v_val_array = v_val_array[:count]\n",
    "print(x_val_array.shape, y_val_array.shape, v_val_array.shape, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11,
     28,
     86
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For training the Pytorch Model for Imitating the result with Walnut Slice Huber\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "import torch.optim as optim\n",
    "from skimage import measure\n",
    "\n",
    "def get_ssim(pred, ground):\n",
    "    ssim_array = []\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        t1 = np.min(ground[i].flatten())\n",
    "        t2 = np.max(ground[i].flatten())\n",
    "        reference_image = (ground[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        t1 = np.min(pred[i, 0, :, :].flatten())\n",
    "        t2 = np.max(pred[i, 0, :, :].flatten())\n",
    "        distorted_image = (pred[i, 0]-t1)*255/(t2-t1)\n",
    "        \n",
    "        ssim_temp = measure.compare_ssim(distorted_image, reference_image, data_range=255)\n",
    "        ssim_array.append(ssim_temp)\n",
    "    \n",
    "    return ssim_array\n",
    "\n",
    "def get_ssim_distribution(ssim_input_array, value_array):\n",
    "    ssim_array = {}\n",
    "    ssim_array[0.01] = []\n",
    "    ssim_array[0.02] = []\n",
    "    ssim_array[0.03] = []\n",
    "    ssim_array[0.04] = []\n",
    "    ssim_array[0.05] = []\n",
    "    \n",
    "    for i in range(len(value_array)):\n",
    "        value     = value_array[i][0]\n",
    "        ssim_temp = ssim_input_array[i]\n",
    "        if value < 0.01:\n",
    "            ssim_array[0.01].append(ssim_temp)\n",
    "        elif value < 0.02:\n",
    "            ssim_array[0.02].append(ssim_temp)\n",
    "        elif  value < 0.03:\n",
    "            ssim_array[0.03].append(ssim_temp)\n",
    "        elif  value < 0.04:\n",
    "            ssim_array[0.04].append(ssim_temp)\n",
    "        elif value < 0.05:\n",
    "            ssim_array[0.05].append(ssim_temp)\n",
    "    \n",
    "    mean_ssim_array = []\n",
    "    for k in ssim_array.keys():\n",
    "        mean_ssim_array.append(np.mean(ssim_array[k]))\n",
    "    mean_ssim_array = [str(round(x, 5)) for x in mean_ssim_array]\n",
    "    \n",
    "    return \", \".join(mean_ssim_array)\n",
    "\n",
    "model = MyUnet_half()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "device    = torch.device(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.L1Loss()\n",
    "mse_criterion  = nn.MSELoss()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(torch.abs((output - target)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "prev_min   = 0\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    idx     = np.random.permutation(len(x_array))\n",
    "    x_array = x_array[idx]\n",
    "    y_array = y_array[idx]\n",
    "    v_array = v_array[idx]\n",
    "    \n",
    "    # Train Loop\n",
    "    for i in range(len(x_array)//batch_size):\n",
    "        x = x_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        v = v_array[i*batch_size:(i+1)*batch_size, :]\n",
    "        v1 = v\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "        v2 = v\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, v)\n",
    "        \n",
    "        loss  =  my_loss(output, y)\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        v      = v.data.cpu().numpy()\n",
    "        v3 = v\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        \n",
    "        for ik, vt in enumerate(v):\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    print('Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    ssim_array = []\n",
    "    value_array  = []\n",
    "    \n",
    "    # Validation Loop\n",
    "    for i in range(len(x_val_array)//batch_size):\n",
    "        x = x_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = y_val_array[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        v = v_val_array[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        v = torch.tensor(v, device=device).float()\n",
    "\n",
    "        output = model(x, v)\n",
    "\n",
    "        loss = my_loss(output, y)\n",
    "        \n",
    "        output = output.data.cpu().numpy()\n",
    "        y      = y.data.cpu().numpy()\n",
    "        v      = v.data.cpu().numpy()\n",
    "        \n",
    "        ssim_values = get_ssim(output, y)\n",
    "        \n",
    "        for vt in ssim_values:\n",
    "            ssim_array.append(vt)\n",
    "        for vt in v:\n",
    "            value_array.append(vt)\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "    \n",
    "    \n",
    "    ssim_string = get_ssim_distribution(ssim_array, value_array)\n",
    "    ssim_mean   = np.mean(ssim_array)\n",
    "    print(epoch, \" SSIM mean \", ssim_mean, 'Loss ', np.mean(loss_array), ' SSIM ', ssim_string)\n",
    "    \n",
    "    if ssim_mean > prev_min:\n",
    "        prev_min = ssim_mean\n",
    "        print('saving the model ', prev_min)\n",
    "        torch.save(model.state_dict(), \"unethalf-huber-walnut.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
